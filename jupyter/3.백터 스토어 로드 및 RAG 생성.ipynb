{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6445bbb8-e219-4a75-8fa4-58928b6a0d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a818cc0-209f-4fdc-ac1c-0b5d4242f28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docmesh.embedding import EmbeddingModelFactory\n",
    "from docmesh.vector_store import VectorStoreFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1ad75c1-b037-4179-a8c0-05ebe3a3ccd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    }
   ],
   "source": [
    "# EmbeddingModelFactory를 통해 임베딩 모델 생성\n",
    "embedding_model = EmbeddingModelFactory.create_embedding_model(\n",
    "    provider=\"openai\", model_name=\"text-embedding-ada-002\"\n",
    ")\n",
    "# VectorStoreFactory를 통해 벡터 스토어 생성\n",
    "vector_store = VectorStoreFactory.create_vector_store(\n",
    "    provider=\"faiss\", embedding_model=embedding_model, path=\"langchain_doc.faiss\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20db3147-8acf-44df-a078-a97cb2d3b131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    }
   ],
   "source": [
    "vector_store.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb8af38e-a141-4dbd-9e55-6a32e0bc997d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id='7962ef82-f28f-45f1-a0eb-b8bf3e989e64', metadata={'h2': 'Next Steps', 'source': 'https://python.langchain.com/docs/tutorials/chatbot/'}, page_content='Next Steps \\u200b  \\nNow that you understand the basics of how to create a chatbot in LangChain, some more advanced tutorials you may be interested in are:  \\n: Enable a chatbot experience over an external source of data  \\nConversational RAG  \\n: Build a chatbot that can take actions  \\nAgents  \\nIf you want to dive deeper on specifics, some things worth checking out are:  \\n: streaming is   for chat applications  \\nStreaming  \\ncrucial  \\n: for a deeper dive into all things related to message history  \\nHow to add message history  \\n: more techniques for managing a large chat history  \\nHow to manage large message history  \\n: for more detail on building with LangGraph  \\nLangGraph main docs  \\nEdit this page Was this page helpful? Previous  \\nBuild a simple LLM application with chat models and prompt templates  \\nNext  \\nBuild a Retrieval Augmented Generation (RAG) App: Part 2  \\nOverview  \\nSetup  \\nJupyter Notebook  \\nInstallation  \\nLangSmith  \\nQuickstart  \\nMessage persistence  \\nPrompt templates  \\nManaging Conversation History  \\nStreaming  \\nNext Steps  \\nCommunity  \\nTwitter  \\nGitHub  \\nOrganization  \\nPython  \\nJS/TS  \\nMore  \\nHomepage  \\nBlog  \\nYouTube  \\nCopyright © 2025 LangChain, Inc.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = vector_store.search(\"what is rag?\")\n",
    "query[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a42fd3c7-6d8f-48e4-87f2-d50cc1962a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docmesh.llm import LLMFactory\n",
    "from docmesh.config import Config\n",
    "from docmesh.qa_bot import QAService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf0bb4bd-9565-420f-8e04-105ecdf7f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. LLM 서비스 생성 (팩토리 사용)\n",
    "llm = LLMFactory.create_llm_service(\n",
    "    provider=Config.LLM_PROVIDER,\n",
    "    model=Config.LLM_MODEL,\n",
    "    temperature=Config.LLM_TEMPERATURE,\n",
    ")\n",
    "\n",
    "retriever = vector_store.vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7e1992e-63f0-47f0-840c-01d92b306d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'langchain은 LangChain 프레임워크의 일부로, LangGraph와 함께 상태 유지형, 다중 액터 애플리케이션을 구축하는 데 사용되는 도구입니다. LangGraph은 LangChain과 원활하게 통합되지만 독립적으로도 사용할 수 있습니다. LangGraph는 링크드인, 우버, 클라르나, 깃랩 등에서 신뢰할 수 있는 프로덕션 급 에이전트를 구축하는 데 사용됩니다.\\n\\nSources:\\n- https://python.langchain.com/docs/integrations/providers/\\n- https://python.langchain.com/docs/introduction/\\n- https://python.langchain.com/docs/tutorials/graph/'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot = QAService(llm, retriever)\n",
    "bot.answer_question(\"langchain은 무엇입니까?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5991d1ef-fdf8-47ae-ba8f-b3122abd70f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
